{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":73291,"databundleVersionId":8930475,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Load Python Pakages","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#basics\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport time\nimport matplotlib.pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#preprocessing\n\n#feature engineering\nfrom sklearn.feature_selection import mutual_info_classif\n\n\n#algorithms\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n#model evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.metrics import RocCurveDisplay, confusion_matrix\nfrom sklearn.metrics import roc_curve, confusion_matrix, ConfusionMatrixDisplay\n\n#!pip3 install yellowbrick\nfrom yellowbrick.features import FeatureImportances\nfrom yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC, DiscriminationThreshold\n\n#!pip3 install shap\nimport shap\n\nfrom ipywidgets import interact, FloatSlider\n\nrandom_state = 42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preprocessing\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer, MinMaxScaler, LabelEncoder,OneHotEncoder, OrdinalEncoder\n\n#feature engineering\nfrom sklearn.feature_selection import mutual_info_classif\n\n\n#transformers and pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.base import clone\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\nfrom sklearn import set_config\n\n#algorithms\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.base import clone\nfrom lightgbm import early_stopping\nfrom lightgbm import log_evaluation\nfrom sklearn.linear_model import LogisticRegression\n\n\n#model evaluation\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, log_loss, auc, accuracy_score, balanced_accuracy_score\nfrom sklearn.metrics import make_scorer, RocCurveDisplay, confusion_matrix\n\n# Optuna and visualization tools\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.visualization import plot_contour\nfrom optuna.visualization import plot_edf\nfrom optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_slice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets look into data","metadata":{}},{"cell_type":"code","source":"# Read the data\ntrain_df = pd.read_csv('/kaggle/input/playground-series-s4e7/train.csv', index_col=[0])\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e7/test.csv', index_col=[0])\n#original_df = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/train.csv', index_col=[0])\n\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This function reduces memory usage. Reference: https://www.kaggle.com/code/jmascacibar/optimizing-memory-usage-with-insurance-cross-sell/notebook\ndef shrink_and_dummify(df):\n    df[\"Vehicle_Age\"] = df[\"Vehicle_Age\"].apply(lambda x: 0 if x == '< 1 Year' else (1 if x == '1-2 Year' else 2)).astype('int8')\n    df['Gender'] = df['Gender'].apply(lambda x: 0 if x == 'Female' else 1).astype('int8')\n    df['Vehicle_Damage'] = df['Vehicle_Damage'].apply(lambda x: 0 if x == 'No' else 1).astype('int8')\n    df['Age'] = df['Age'].astype('int8')\n    df['Driving_License'] = df['Driving_License'].astype('int8')\n    df['Region_Code'] = df['Region_Code'].astype('int8')\n    df['Previously_Insured'] = df['Previously_Insured'].astype('int8')\n    df['Annual_Premium'] = df['Annual_Premium'].astype('int32')\n    df['Policy_Sales_Channel'] = df['Policy_Sales_Channel'].astype('int16')\n    df['Vintage'] = df['Vintage'].astype('int16')\n    \n    if 'Response' in df.columns:\n        df['Response'] = df['Response'].astype('int8')\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = shrink_and_dummify(train_df)\ntest_df = shrink_and_dummify(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Descpriptive statistics","metadata":{}},{"cell_type":"code","source":"train_df.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\npalette_color = sns.color_palette('pastel')\nexplode = [0.05 for _ in range(train_df['Response'].nunique())]\n\n# Plotting\ntrain_df.groupby('Response')['Response'].count().plot.pie(\n    colors=palette_color,\n    explode=explode,\n    autopct=\"%1.1f%%\",\n    shadow=True,  # Adding shadow for better visibility\n    startangle=140,  # Start angle for better alignment\n    textprops={'fontsize': 14},  # Adjust text size\n    wedgeprops={'edgecolor': 'black', 'linewidth': 1.5}  # Adding edge color and width\n)\n\n# Adding a title\nplt.title('Target Distribution', fontsize=18, weight='bold')\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')\n\n# Displaying the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Grouping features for preprocessing purposes","metadata":{}},{"cell_type":"code","source":"train_df.nunique().sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store numerical and categorical features to different lists for visualization purposes\nfeature_list = [feature for feature in train_df.columns if not feature  == \"Response\"]\n\ntarget = \"Response\"\n\nbinary_features = ['Previously_Insured','Driving_License', 'Gender', 'Vehicle_Damage', 'Vehicle_Age']\n\ncontinuous_features = list(set(feature_list) - set(binary_features))\n\nassert feature_list.sort() == (continuous_features + binary_features).sort()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"#Data is huge just take some sample for eda\neda_df = train_df.sample(frac=0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 3, figsize=(30, 20))\nfor var, subplot in zip(continuous_features, ax.flatten()):\n    sns.boxplot(x='Response', y=var, data=eda_df, ax=subplot, palette='Set3')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 3, figsize=(30, 20))\nfor var, subplot in zip(binary_features, ax.flatten()):\n    sns.barplot(x= var, y= 'Response', data=eda_df, ax=subplot, palette='Set3')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"# determine the mutual information for features\n\nmutual_df = eda_df[feature_list]\ny_sampled = eda_df.Response\nmutual_info = mutual_info_classif(mutual_df, y_sampled, random_state=random_state)\n\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = mutual_df.columns\nmutual_info = pd.DataFrame(mutual_info.sort_values(ascending=False), columns = [\"Numerical_Feature_MI\"] )\nmutual_info.style.background_gradient(\"hot\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#eda_df = pd.get_dummies(eda_df, columns=['Previously_Insured'])\n#eda_df['Previously_Insured_Yes'] = eda_df['Previously_Insured_Yes'].astype('int')\n#eda_df['Previously_Insured_No'] = eda_df['Previously_Insured_No'].astype('int')\n#eda_df.head().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#eda_df = eda_df.rename(columns={'Vehicle_Age_1-2 Year': 'Vehicle_Age_1-2_Year', 'Vehicle_Age_< 1 Year': 'Vehicle_Age_less_than_1_Year',\n#                                'Vehicle_Age_> 2 Years': 'Vehicle_Age_greater_than_2_Years'})\n#eda_df.head().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#eda_df['Vehicle_Age'] = eda_df['Vehicle_Age'].apply(lambda x: '1-2_Year' if x == '1-2 Year' else ('less_than_1_Year' \n#                                                                  if x == '< 1 Year' else ('greater_than_2_Years')))\n#eda_df.head().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#eda_df = pd.get_dummies(eda_df, columns=['Vehicle_Age'])\n#eda_df['Vehicle_Age_less_than_1_Year'] = eda_df['Vehicle_Age_less_than_1_Year'].astype('int')\n#eda_df['Vehicle_Age_1-2_Year'] = eda_df['Vehicle_Age_1-2_Year'].astype('int')\n#eda_df['Vehicle_Age_greater_than_2_Years'] = eda_df['Vehicle_Age_greater_than_2_Years'].astype('int')\n#eda_df.head().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#eda_df = pd.get_dummies(eda_df, columns=['Gender'])\n#eda_df['Gender_Male'] = eda_df['Gender_Male'].astype('int')\n#eda_df['Gender_Female'] = eda_df['Gender_Female'].astype('int')\n#eda_df.head().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#eda_df = pd.get_dummies(eda_df, columns=['Vehicle_Damage'])\n#eda_df['Vehicle_Damage_Yes'] = eda_df['Vehicle_Damage_Yes'].astype('int')\n#eda_df['Vehicle_Damage_No'] = eda_df['Vehicle_Damage_No'].astype('int')\n#eda_df.head().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modeling and hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#A cross-validation function with early stopping. I used some crowdy code  to get the best iterations. If you have more elegant suggestions, please put the comment.\n\ndef cross_validate_score(model, data: pd.DataFrame, cv=None, test_data: pd.DataFrame = None, label: str = 'Response', include_original: bool = True, original_data: pd.DataFrame = None):\n    if cv is None:\n        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    if test_data is None:\n        raise ValueError(\"test_data must be provided\")\n    \n    X = data.copy()\n    y = X.pop(label)\n    \n    val_predictions = np.zeros(len(X))\n    test_predictions = np.zeros(len(test_data))\n    train_scores, val_scores = [], []\n    \n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        X_train = X.iloc[train_idx].reset_index(drop=True)\n        y_train = y.iloc[train_idx].reset_index(drop=True)\n        X_val = X.iloc[val_idx].reset_index(drop=True)\n        y_val = y.iloc[val_idx].reset_index(drop=True)\n        \n        if include_original:\n            if original_data is None:\n                raise ValueError(\"original_data must be provided when include_original is True\")\n            X_train = pd.concat([original_data.drop(label, axis=1), X_train]).reset_index(drop=True)\n            y_train = pd.concat([original_data[label], y_train]).reset_index(drop=True)\n        \n        model_cloned = clone(model)\n        \n        if isinstance(model_cloned, XGBClassifier):\n            eval_set = [(X_val, y_val)]\n            model_cloned.fit(X_train, y_train, eval_set=eval_set, early_stopping_rounds=50, verbose=False)\n            best_iteration = model_cloned.best_iteration\n            train_preds_proba = model_cloned.predict_proba(X_train, iteration_range=(0, best_iteration))[:, 1]\n            val_preds_proba = model_cloned.predict_proba(X_val, iteration_range=(0, best_iteration))[:, 1]\n            test_preds_proba = model_cloned.predict_proba(test_data, iteration_range=(0, best_iteration))[:, 1]\n        elif isinstance(model_cloned, LGBMClassifier):\n            eval_set = [(X_val, y_val)]\n            model_cloned.fit(X_train, y_train, eval_set=eval_set, eval_metric='auc', callbacks=[early_stopping(50)])\n            best_iteration = model_cloned.best_iteration_\n            train_preds_proba = model_cloned.predict_proba(X_train, num_iteration=best_iteration)[:, 1]\n            val_preds_proba = model_cloned.predict_proba(X_val, num_iteration=best_iteration)[:, 1]\n            test_preds_proba = model_cloned.predict_proba(test_data, num_iteration=best_iteration)[:, 1]\n        elif isinstance(model_cloned, CatBoostClassifier):\n            model_cloned.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=False)\n            best_iteration = model_cloned.get_best_iteration()\n            train_preds_proba = model_cloned.predict_proba(X_train)[:, 1]\n            val_preds_proba = model_cloned.predict_proba(X_val)[:, 1]\n            test_preds_proba = model_cloned.predict_proba(test_data)[:, 1]\n        else:\n            raise ValueError(\"Model type not supported for early stopping.\")\n        \n        val_predictions[val_idx] = val_preds_proba\n        train_scores.append(roc_auc_score(y_train, train_preds_proba))\n        val_scores.append(roc_auc_score(y_val, val_preds_proba))\n        \n        print(f'Fold {fold}: {val_scores[-1]:.5f}')\n        \n        test_predictions += test_preds_proba / cv.get_n_splits()\n        \n    print(f'Val Score: {np.mean(val_scores):.7f} ± {np.std(val_scores):.7f} | Train Score: {np.mean(train_scores):.7f} ± {np.std(train_scores):.7f} | {label}')\n    \n    return val_scores, val_predictions, test_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_summary, oof_predictions, test_predictions = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"code","source":"xgb_params = {\n    \n    'n_estimators': 10000,\n    'eta': 0.05,\n    'alpha':  0.2545607592482198,\n    'subsample': 0.8388163485383147, \n    'colsample_bytree': 0.2732499701466825, \n    'max_depth': 16,\n    'min_child_weight': 5,\n    'gamma': 0.0017688666476104672,\n    'eval_metric': 'auc',\n    'max_bin': 262143, #a weird max_bin, for reference: https://www.kaggle.com/competitions/playground-series-s4e7/discussion/516265   \n    'tree_method': 'gpu_hist',\n}\n\nxgb_tuned = XGBClassifier(**xgb_params, random_state=random_state)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncv_summary['xgb'], oof_predictions['xgb'], test_predictions['xgb'] = cross_validate_score(xgb_tuned, data=train_df, test_data=test_df, include_original=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CatBoost","metadata":{}},{"cell_type":"code","source":"catb_params = {\n    'iterations': 10000,\n    'eval_metric': 'AUC',\n    'task_type': 'GPU',\n    'learning_rate': 0.1,\n    'depth': 9,\n    'l2_leaf_reg': 55.37964307854247,\n    'max_bin': 404,\n    'bagging_temperature': 0.017138393608280057,\n    'random_strength': 9.256288011643901\n}\n\n\n#defining all features as cateegorical makes huge impact \n#reference: https://www.kaggle.com/code/rohanrao/automl-grand-prix-1st-place-solution\ncatb_tuned = CatBoostClassifier(**catb_params, random_state=random_state, logging_level='Silent',cat_features=test_df.columns.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncv_summary['catb'], oof_predictions['catb'], test_predictions['catb'] = cross_validate_score(catb_tuned, data=train_df, test_data=test_df, include_original=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Neural Network","metadata":{}},{"cell_type":"code","source":"def cross_validate_score_nn(model_builder, data: pd.DataFrame, test_data: pd.DataFrame, label: str = 'Response', cv=None):\n    if cv is None:\n        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    X = data.copy()\n    y = X.pop(label)\n    \n    val_predictions = np.zeros(len(X))\n    test_predictions = np.zeros(len(test_data))\n    val_scores = []\n    \n    input_shape = {feature: int(data[feature].max()) for feature in categorical_features}\n    \n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        X_train = X.iloc[train_idx].reset_index(drop=True)\n        y_train = y.iloc[train_idx].reset_index(drop=True)\n        X_val = X.iloc[val_idx].reset_index(drop=True)\n        y_val = y.iloc[val_idx].reset_index(drop=True)\n        \n        model = model_builder(input_shape)\n        model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n                      loss='binary_crossentropy',\n                      metrics=[keras.metrics.AUC(name='auc')])\n        \n        X_train_inputs = {feature: X_train[feature].values for feature in categorical_features}\n        X_train_inputs['numerical'] = X_train[numerical_features].values\n        \n        X_val_inputs = {feature: X_val[feature].values for feature in categorical_features}\n        X_val_inputs['numerical'] = X_val[numerical_features].values\n        \n        model.fit(X_train_inputs, y_train, epochs=4, batch_size=1024, validation_data=(X_val_inputs, y_val), verbose=0)\n\n        val_preds_proba = model.predict(X_val_inputs).flatten()\n        val_predictions[val_idx] = val_preds_proba\n        val_scores.append(roc_auc_score(y_val, val_preds_proba))\n        \n        # Predict on test data\n        test_inputs = {feature: test_data[feature].values for feature in categorical_features}\n        test_inputs['numerical'] = test_data[numerical_features].values\n        test_preds_proba = model.predict(test_inputs).flatten()\n        test_predictions += test_preds_proba / cv.get_n_splits()\n        \n        print(f'Fold {fold}: {val_scores[-1]:.5f}')\n    \n    print(f'Val Score: {np.mean(val_scores):.7f} ± {np.std(val_scores):.7f} | {label}')\n    \n    return val_scores, val_predictions, test_predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data curation to deal with high cardinality with embedding layers.\ndef prepare_data(train_df, test_df, numerical_features, categorical_features):\n    # Concatenate train and test data for consistent encoding\n    combined_data = pd.concat([train_df, test_df], keys=['train', 'test']).reset_index(level=0)\n    \n    # Ordinal encode categorical features\n    ordinal_encoder = OrdinalEncoder()\n    combined_data[categorical_features] = ordinal_encoder.fit_transform(combined_data[categorical_features])\n    \n    # Split back into train and test sets\n    train_data = combined_data[combined_data['level_0'] == 'train'].drop(columns=['level_0'])\n    test_data = combined_data[combined_data['level_0'] == 'test'].drop(columns=['level_0'])\n    \n    return train_data, test_data\n\n# Example usage\nnumerical_features = ['Driving_License', 'Previously_Insured']\ncategorical_features = ['Region_Code', 'Policy_Sales_Channel', 'Vintage', 'Annual_Premium', 'Age', 'Gender', 'Vehicle_Damage', 'Vehicle_Age']\n\ntrain_data, test_data = prepare_data(train_df, test_df, numerical_features, categorical_features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reference: https://www.kaggle.com/code/paddykb/ps-s4e7-keras-haz-insurance-losses/notebook\n#NN with embedding layers to  deal with high cardinality\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\ndef create_model(input_shape):\n    inputs = []\n    flat_embeddings = []\n\n    for feature, input_dim in input_shape.items():\n        output_dim = min(64, round(1.6 * (input_dim + 1) ** 0.56))  \n        input_layer = keras.Input(shape=(1,), name=feature)\n        embedding_layer = layers.Embedding(input_dim=input_dim + 1, output_dim=output_dim)(input_layer)  \n        embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n        embedding_layer = layers.Flatten()(embedding_layer)\n        inputs.append(input_layer)\n        flat_embeddings.append(embedding_layer)\n        \n    numerical_input = keras.Input(shape=(len(numerical_features),), name='numerical')\n    inputs.append(numerical_input)\n\n    concatenated_inputs = layers.Concatenate()(flat_embeddings + [numerical_input])\n    concatenated_inputs_bn = layers.BatchNormalization()(concatenated_inputs)\n\n    x = layers.Dense(256, activation='mish')(concatenated_inputs_bn)\n    x = layers.BatchNormalization()(x)\n    x = layers.Concatenate()([x, concatenated_inputs_bn])\n    x = layers.Dense(128, activation='mish')(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.BatchNormalization()(x)\n\n    outputs = layers.Dense(1, activation='sigmoid')(x)\n\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncv_summary['nn'], oof_predictions['nn'], test_predictions['nn'] = cross_validate_score_nn(create_model, train_data, test_data=test_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Performance Summary","metadata":{}},{"cell_type":"code","source":"#performance summary for base learners\ntransposed_df = cv_summary.transpose()\ntransposed_df.columns = ['fold1','fold2','fold3','fold4','fold5']\ntransposed_df['Mean'] = transposed_df.mean(axis=1)\ntransposed_df['Std'] = transposed_df.std(axis=1)\ntransposed_df.sort_values(by = 'Mean', ascending=False).style.background_gradient('Dark2_r')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def use_models_from_previous_version(df1, df2):\n    df2 = df2.rename(columns={'xgb': 'xgb_01'})\n    df2 = df2.rename(columns={'catb': 'catb_01'})\n    merged_df = pd.concat([df1.reset_index(drop=True), df2.reset_index(drop=True)], axis=1)\n    merged_df\n    return merged_df\n\noof_predictions = use_models_from_previous_version(oof_predictions, oof_preds_version1)\ntest_predictions = use_models_from_previous_version(test_predictions, test_preds_version1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.2, style=\"whitegrid\")\ncorrelation_train = oof_predictions.corr()\nmask = np.triu(np.ones_like(correlation_train, dtype=bool))\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train, \n            mask=mask, \n            annot=True, \n            fmt='.3f', \n            cmap='coolwarm', \n            square=True, \n            linewidths=.5, \n            cbar_kws={\"shrink\": .75})\nplt.title('Model Diversity Check - Correlation Heatmap', fontsize=20, pad=20)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Voting vs Stacking","metadata":{}},{"cell_type":"code","source":"#probability averaging\ndef soft_voting_ensemble(oof_df, y_actual):\n\n    mean_prob = oof_df.mean(axis=1)\n    ensemble_auc = roc_auc_score(y_actual, mean_prob)\n    print(f'Ensemble AUC: {ensemble_auc:.5f}')\n    \n    return mean_prob, ensemble_auc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ , ensemble_auc = soft_voting_ensemble( oof_predictions, train_df.Response)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#parameters suggested for meta model                                                                                                 \nmeta_model_params = {\n'n_estimators': 43, 'alpha': 0.000759453839369262, 'subsample': 0.8635904939859487, 'colsample_bytree': 0.7579443772400538,\n    'max_depth': 7, 'min_child_weight': 5, 'learning_rate': 0.13688008280542863, 'gamma': 0.19965095682630274}\n\nmeta_model = XGBClassifier(**meta_model_params, random_state=random_state)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\n\n#Deciding which models to include ensemble\n#from sklearn.feature_selection import RFECV\n\nmin_features_to_select = 1\n\n# Create a pipeline with preprocessor, RFECV, and LGBMClassifier\n#pipeline = Pipeline([\n#    ('rfecv', RFECV(estimator=meta_model,\n#                    step=1,\n#                    cv=cv,\n#                    scoring=\"roc_auc\",\n#                    min_features_to_select=min_features_to_select,\n#                    n_jobs=-1,))\n#])\n\n# Fit the pipeline on the training data\n#pipeline.fit(oof_predictions, train_df.Response)\n\n#CV score\n#print(\"Best CV score: \")\n#selected_models = np.array( oof_predictions.columns)[pipeline.named_steps['rfecv'].support_]\n#print( pipeline.named_steps['rfecv'].cv_results_[\"mean_test_score\"][len(selected_models) - 1])\n\n\n\n# Selected features after RFECV\n#print('Number of evaluated models:', len(oof_predictions.columns))\n#print('Number of selected models for ensemble:', len(selected_models))\n#print(\"Selected models:\", selected_models)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#above code selects following models:\nselected_models = ['xgb_01', 'catb_01', 'nn', 'catb']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"meta_model = meta_model.fit(oof_predictions[selected_models], train_df.Response)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_test =  meta_model.predict_proba(test_predictions[selected_models])[:, 1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'id': test_df.index,\n                       'Response': preds_test})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use data leakage\n#reference: https://www.kaggle.com/competitions/playground-series-s4e7/discussion/520253\n#I guess it can be written much more elegant. \nreversed_labels = test_df.join(original_df.set_index(list(test_df.columns)),on=list(test_df.columns),how='inner')\ndf_assign =  1 - reversed_labels['Response']\nsub = output.set_index('id')\nsub['id'] = sub.index\nsub = sub[['id', 'Response']]\nsub['Response'].loc[df_assign.index] = df_assign\nsub.to_parquet('submission.parquet', index=False)\nsub.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_predictions.to_parquet('oof_predictions.parquet', index=False)\ntest_predictions.to_parquet('test_predictions.parquet', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}