{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":76727,"databundleVersionId":9045607,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Python Pakages","metadata":{}},{"cell_type":"code","source":"#basics\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport time\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom sklearn.base import clone\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import QuantileTransformer, quantile_transform\n\n#statistics\nfrom scipy.stats import randint, mode\n\n#feature engineering\nfrom sklearn.feature_selection import mutual_info_classif\n\n#transformers and pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\nfrom sklearn import set_config\n\n#feature engineering\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import RFECV\n\n#algorithms\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier, Pool\nfrom lightgbm import LGBMClassifier\nfrom lightgbm.callback import early_stopping, log_evaluation\nfrom sklearn.linear_model import LogisticRegression\n\n#model evaluation\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, log_loss, auc, accuracy_score, balanced_accuracy_score\nfrom sklearn.metrics import make_scorer, RocCurveDisplay, confusion_matrix\n\n#model evaluation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, confusion_matrix, matthews_corrcoef, make_scorer\n\n# Optuna and visualization tools\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.visualization import plot_contour\nfrom optuna.visualization import plot_edf\nfrom optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_slice\n\nrandom_state = 42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets look into data","metadata":{}},{"cell_type":"code","source":"# Read the data\ntrain_df = pd.read_csv('/kaggle/input/playground-series-s4e8/train.csv', index_col=[0])\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e8/test.csv', index_col=[0])\n\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check for missing values","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nmsno.matrix(train_df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing = pd.DataFrame(train_df.isnull().sum().sort_values(ascending=False))\nmissing.columns = [\"missing_count\"]\n#missing = missing.loc[(missing!=0).any(axis=1)]\n#missing[\"missing_percent\"] = missing[0:] / len(train_df)*100\nmissing[\"missing_percent\"] = missing / len(train_df)*100\nmissing.style.background_gradient('viridis')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Descriptive statistics","metadata":{}},{"cell_type":"code","source":"#numerical feature descriptive statistics\n\ntrain_df.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Target frequency\n\nplt.figure(figsize=(10, 10))\npalette_color = sns.color_palette('pastel')\nexplode = [0.02 for _ in range(train_df['class'].nunique())]\n\n# Plotting\ntrain_df.groupby('class')['class'].count().plot.pie(\n    colors=palette_color,\n    explode=explode,\n    autopct=\"%1.1f%%\",\n    shadow=True,  # Adding shadow for better visibility\n    startangle=140,  # Start angle for better alignment\n    textprops={'fontsize': 14},  # Adjust text size\n    wedgeprops={'edgecolor': 'black', 'linewidth': 1.5}  # Adding edge color and width\n)\n\n# Adding a title\nplt.title('Class Distribution', fontsize=18, weight='bold')\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')\n\n# Displaying the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\ntrain_df['class'] = le.fit_transform(train_df['class'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Grouping features for preprocessing purposes","metadata":{}},{"cell_type":"code","source":"train_df.nunique().sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store numerical and categorical features to different lists for visualization purposes\nfeature_list = [feature for feature in train_df.columns if not feature  == \"class\"]\n\ntarget = \"class\"\n\nnumerical_features = ['stem-height', 'cap-diameter', 'stem-width']\n\ncategorical_features = list(set(feature_list) - set(numerical_features))\n\nassert feature_list.sort() == (numerical_features + categorical_features).sort()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda_df = train_df.sample(frac= 0.1, random_state=random_state)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 3, figsize=(30, 10))\nfor var, subplot in zip(numerical_features, ax.flatten()):\n    sns.boxplot(x='class', y=var, data=eda_df, ax=subplot, palette='Set3')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check for cardinality\ntrain_df[categorical_features].nunique().sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Categoricals\n#Just visualize moderate cardinality features\nmoderate_cardinality_features = ['season', 'veil-type', 'has-ring', 'veil-color']\n\nfig, ax = plt.subplots(2, 2, figsize=(30, 30))\nfor var, subplot in zip(moderate_cardinality_features, ax.flatten()):\n    sns.barplot(x=var,y= 'class',  data=eda_df, ax=subplot, palette='Set3')\n    subplot.set_xticklabels(subplot.get_xticklabels(), rotation=45, ha='right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Very strong features. Some categories are dedicated to one class. For example, if the vell-type is 'l,' it is non-poisonous. If 't', it is poisonous.","metadata":{}},{"cell_type":"code","source":"# Mutual Information score\ny_sampled = eda_df['class']\nmutual_df = eda_df[numerical_features]\n\nmutual_info = mutual_info_classif(mutual_df.fillna(0), y_sampled, random_state=random_state)\n\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = mutual_df.columns\nmutual_info = pd.DataFrame(mutual_info.sort_values(ascending=False), columns = [\"Numerical_Feature_MI\"] )\nmutual_info.style.background_gradient(\"cool\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mutual_df_categorical = eda_df[categorical_features]\n#categorical features must be encoded to get mutual information\nfor colname in mutual_df_categorical:\n    mutual_df_categorical[colname], _ = mutual_df_categorical[colname].factorize()\nmutual_info = mutual_info_classif(mutual_df_categorical.fillna(\"Do_not_have_feature\"), y_sampled, random_state=1)\n\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = mutual_df_categorical.columns\npd.DataFrame(mutual_info.sort_values(ascending=False), columns = [\"Categorical_Feature_MI\"] ).style.background_gradient(\"cool\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pair-plot for most important features\nsns.pairplot(eda_df[numerical_features + [\"class\"]], hue=\"class\",  corner=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"code","source":"#Converts object types to category\n#reference for why we would do that?: https://catboost.ai/en/docs/concepts/speed-up-training\ntrain_df[categorical_features] = train_df[categorical_features].astype('category')\ntest_df[categorical_features] = test_df[categorical_features].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Catboost complains about missing value format\ndef preprocess_catboost(train_df, test_data, cat_features):\n    for col in cat_features:\n        train_df[col] = train_df[col].astype(str).fillna('NaN')\n        test_data[col] = test_data[col].astype(str).fillna('NaN')\n    return train_df, test_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For XGB\n#Check this discussion to see how XGB fails to handle cat features:\n#https://www.kaggle.com/competitions/playground-series-s4e8/discussion/523781#2945249\nencoder  = ColumnTransformer(remainder='passthrough',\n    transformers=[\n        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_features),\n    ])\n\nencoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modeling","metadata":{}},{"cell_type":"code","source":"#seperate target\ny = train_df['class']\ntrain_df = train_df.drop(['class'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CV strategy\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_validate_score(model, train_df, y, cv, test_data):\n    val_scores = []\n    test_preds = np.zeros((test_data.shape[0],))\n    oof_preds = np.zeros((train_df.shape[0],))\n\n    if isinstance(model, CatBoostClassifier):\n        cat_features = model.get_params().get('cat_features', [])\n        train_df, test_data = preprocess_catboost(train_df, test_data, cat_features)\n\n    for fold, (train_idx, val_idx) in enumerate(cv.split(train_df, y)):\n        X_train = train_df.iloc[train_idx].reset_index(drop=True)\n        y_train = y.iloc[train_idx].reset_index(drop=True)\n        \n        X_val = train_df.iloc[val_idx].reset_index(drop=True)\n        y_val = y.iloc[val_idx].reset_index(drop=True)\n        \n        model = clone(model)\n        \n        eval_set = [(X_val, y_val)]\n\n        if isinstance(model, LGBMClassifier):\n            model.fit(\n                X_train, y_train,\n                eval_set=eval_set,\n                callbacks=[early_stopping(50)],\n            )\n        elif isinstance(model, CatBoostClassifier):\n            model.fit(\n                X_train, y_train,\n                eval_set=eval_set,\n                early_stopping_rounds=50,\n                verbose=False\n            )\n\n        else:\n            model.fit(\n                X_train, y_train,\n\n            )\n            \n        val_probs = model.predict_proba(X_val)[:, 1]  # Get the probabilities\n        val_preds = (val_probs > 0.5).astype(int)     # Convert probabilities to class labels for MCC\n\n        val_score = matthews_corrcoef(y_val, val_preds)  # Calculate MCC\n        print(f'Fold {fold}: MCC = {val_score:.5f}')\n        \n        val_scores.append(val_score)\n        \n        oof_preds[val_idx] = val_probs  # Store the probabilities for OOF predictions\n\n        test_preds += model.predict_proba(test_data)[:, 1] / cv.get_n_splits()  # Aggregate test probabilities\n\n    mean_val_score = np.mean(val_scores)\n    std_val_score = np.std(val_scores)\n    print(f'Mean Validation MCC: {mean_val_score:.7f}')\n    print(f'Std Validation MCC: {std_val_score:.7f}')\n    \n    return val_scores, test_preds, oof_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_summary, test_preds, oof_preds = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGboost","metadata":{}},{"cell_type":"code","source":"xgb_optuna_params = {\n 'tree_method': 'gpu_hist',\n 'n_estimators': 1696,\n 'alpha': 4.956752183261538e-07,\n 'subsample': 0.7349948172684168,\n 'colsample_bytree': 0.30171411525842506,\n 'max_depth': 15, \n 'min_child_weight': 6,\n 'learning_rate': 0.013301072238797047,\n 'gamma': 5.634602153104516e-08\n}\n\n\nxgb_tuned = XGBClassifier(**xgb_optuna_params, random_state=random_state)\n\n\nxgb_pipeline = make_pipeline(encoder, xgb_tuned)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncv_summary['xgb'], test_preds['xgb'], oof_preds['xgb'] = cross_validate_score(xgb_pipeline, train_df , y,  cv, test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lightgbm","metadata":{}},{"cell_type":"code","source":"lgbm_optuna_params = {\n    'n_estimators': 10000,\n    'learning_rate': 0.02,\n    \"categorical_feature\" : categorical_features,\n    'device': 'gpu',\n    'max_depth': 10,\n    'min_data_in_leaf': 85,\n    'subsample': 0.6720606456166781,\n    'max_bin': 240,\n    'feature_fraction': 0.6946327643448142,\n\n}\n\n\n\nlgbm_tuned = LGBMClassifier(**lgbm_optuna_params, random_state=random_state, verbose=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncv_summary['lgbm'], test_preds['lgbm'], oof_preds['lgbm'] = cross_validate_score(lgbm_tuned, train_df , y,  cv, test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Catboost","metadata":{}},{"cell_type":"code","source":"#Parameters found tuning process by Optuna\ncatb_params = {    \n    \"n_estimators\" : 10000,\n    \"learning_rate\" : 0.075,\n    'cat_features' : categorical_features,\n    'task_type': 'GPU',\n    'random_strength': 0.3718364180573207,\n    'max_bin': 128,\n    'depth': 9,\n    'l2_leaf_reg': 6,\n    'grow_policy': 'SymmetricTree',\n    'boosting_type': 'Plain',\n    'bootstrap_type': 'Bernoulli',\n    'subsample': 0.41936688658110405\n}\n\n# Catb with found hyperparameters\ncatb_tunned = CatBoostClassifier(**catb_params, random_state=random_state)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncv_summary['catb'], test_preds['catb'], oof_preds['catb'] = cross_validate_score(catb_tunned, train_df , y,  cv, test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Performance Summary","metadata":{}},{"cell_type":"code","source":"#performance summary for base learners\ntransposed_df = cv_summary.transpose()\ntransposed_df.columns = ['fold1','fold2','fold3','fold4','fold5']\ntransposed_df['Mean'] = transposed_df.mean(axis=1)\ntransposed_df['Std'] = transposed_df.std(axis=1)\ntransposed_df.sort_values(by = 'Mean', ascending=False).style.background_gradient('Dark2_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model diversity check\n\nsns.set(font_scale=1.1)\ncorrelation_train = oof_preds.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.3f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Voting vs Stacking","metadata":{}},{"cell_type":"markdown","source":"#### Voting","metadata":{}},{"cell_type":"code","source":"#voting hard & soft\ndef voting_ensemble(oof_preds, y, threshold=0.5, voting_type='soft'):\n    if voting_type == 'soft':\n        ensemble_preds = oof_preds.mean(axis=1)\n        ensemble_class_preds = (ensemble_preds > threshold).astype(int)\n        \n    elif voting_type == 'hard':\n        binary_preds = (oof_preds > threshold).astype(int)\n        ensemble_class_preds = mode(binary_preds, axis=1)[0].flatten()\n    \n    mcc_score = matthews_corrcoef(y, ensemble_class_preds)\n    \n    return mcc_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_ensemble(oof_preds, y, voting_type='soft')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_ensemble(oof_preds, y, voting_type='hard')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Stacking","metadata":{}},{"cell_type":"code","source":"#parameters for meta model                                                                                                 \nmeta_model_params = {\n    'C': 0.000237302749626327,\n    'max_iter': 2500,\n    'tol': 9.996751434702547e-05,\n    'solver': 'saga',\n    'penalty': 'l1'\n}\n\nmeta_model = LogisticRegression(**meta_model_params, random_state=random_state)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n#Deciding which models to include ensemble\n\nmin_features_to_select = 1\n\n# Create a pipeline with preprocessor and RFECV\npipeline = Pipeline([\n    \n    ('Scaler', StandardScaler()),\n    ('rfecv', RFECV(estimator=meta_model,\n                    step=1,\n                    cv=cv,\n                    scoring=make_scorer(matthews_corrcoef),\n                    min_features_to_select=min_features_to_select,\n                    n_jobs=-1,))\n])\n\n# Fit the pipeline on oof_preds\npipeline.fit(oof_preds, y)\n\n#CV score\nprint(\"Best CV score: \")\nselected_models = np.array( oof_preds.columns)[pipeline.named_steps['rfecv'].support_]\nprint( pipeline.named_steps['rfecv'].cv_results_[\"mean_test_score\"][len(selected_models) - 1])\n\n\n# Selected models after RFECV\nprint('Number of available models:', len(oof_preds.columns))\nprint('Number of selected models for ensemble:', len(selected_models))\nprint(\"Selected models:\", selected_models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_model = meta_model.fit(oof_preds[selected_models], y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_test =  meta_model.predict(test_preds[selected_models])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_test = le.inverse_transform(preds_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"output = pd.DataFrame({'id': test_df.index,\n                       'class': preds_test})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save oofs and test predictions for later usage\noof_preds.to_parquet('oof_predictions_v01.parquet', index=False)\ntest_preds.to_parquet('test_predictions_v01.parquet', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}